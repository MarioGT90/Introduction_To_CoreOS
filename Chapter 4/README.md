## Chapter 4: Launching a development CoreOS Cluster* Chapter 4 Introduction Slide:  In this chapter, we will create a development CoreOS cluster, deploy our first service, experiment with some CoreOS tools including fleet & etcd, and finally learn how to troubleshoot common container and OS issues.###  0401 Installing dependencies (Docker, Vagrant, fleetctl)* (Chapter 4 Intro Slide) In this video tutorial series, I will be using my Apple computer to demonstrate everything.  If you also have an Apple computer, then you should be able to follow right along with all of the examples.  For those of you running Linux or Unix based operating systems, the demonstrated content should be very similar for your computers.  If you have a Windows based system, then you may have more difficulties, simply because the tools for CoreOS have not been all ported to Windows.* The simplist way to started with CoreOS is to run one or more instances inside virtual machines on your local development computer.  This is obviously not the best for performance and certianly shouldn't be used in production, but it does makes it easy to start experimenting with the platform.* Throughout this tutorial series, we're going to make use of several open source tools: * Vagrant for CoreOS Virtual Machine Management * git for downloading code examples * Docker CLI and Daemon for building/testing Docker images * fleetctl, a CLI tool for managing applications deployed in CoreOS * etcdctl, a CLI tool for managing data within the etcd database* Before we can boot our first CoreOS cluster, we need to make sure we have all pre-requisite software installed.  We're going to be using Vagrant with VirtualBox to manage CoreOS virtual machines on our computer.  Vagrant & VirtualBox should work on every major operating system, but please refer to the Vagrant Documentation on using alternative providers like VMWare: https://docs.vagrantup.com/v2/providers/index.html* To get started, please install Vagrant onto your development workstation.  Installation instructions can be found from the vagrantup.com documenation here: https://docs.vagrantup.com/v2/installation/index.html* I have already installed Vagrant on this computer, as I have access to the vagrant command:```vagrant -v```* Another important software package that we'll be using is Git, which is a software versioning system.  All of the code examples and accompanying documetation for this tutorial series will be hosted on my GitHub account, https://github.com/rosskukulinski in the Introduction To CoreOS repository: https://github.com/rosskukulinski/Introduction_To_CoreOS.  On Apple OSX, you should have the git client pre-installed.  Windows and Linux systems will need to install it if you haven't already.  You can visit the Git website (https://git-scm.com/) for installation directions for your platform.* Once you have Git installed, lets download the accompanying materials for this video tutorial.  ```git clone https://github.com/rosskukulinski/Introduction_To_CoreOS.gitcd Introduction_To_CoreOS```All of the example code for this video tutorial is in this current directory.If you run into any problems or find a bug in any of these exercises, postingan issue on GitHub is a great way to get in touch with me.  Creating an issue is easy:Once you have a GitHub account, simply navigate to this repository, click on Issues,then create new issue.  I'll be notified the moment you create an issue, and I'lltry to respond as quickly as possible.If you've found a bug, and think you know how to fix it, I always appreciatepull requests.Note: Given that CoreOS is in active development, new versions of tools like fleetctl are released regularly, likewise with CoreOS itself and the Vagrantfile. Ensure that you're using the version of the client tools that match the version of CoreOS to which you're connecting.* Now we have everything installed that we need to boot our development CoreOS cluster.###	0402 Boot your first clusterhttp://www.jimmycuadra.com/posts/etcd-2-0-static-bootstrapping-on-coreos-and-vagrant/* In this session, we are going to launch our first CoreOS development cluster.  The Vagrant* We're going to be following the CoreOS Documentation for running CoreOS within a Vagrant virtual machine on your development workstation.  You can find the most up-to-date documentation here: https://coreos.com/docs/running-coreos/platforms/vagrant/```git clone https://github.com/coreos/coreos-vagrant.gitcd coreos-vagrant```Edit user-data and config.rb.  Update to use etcd2, the new database versionWe also need to use the beta channel for this```vagrant upvagrant statusvagrant ssh core-01 -- -Afleetctl list-machinesfleetctl list-machines -fullvagrant haltvagrant destroy```If we want to exit our ssh connection, we can either press CTRL-D, or type ```exit``` and hit enter.  When we exit the SSH session, the CoreOS virtual machines are still running.  This is great for testing and debugging applications as you build them.Vagrant tips: You can SSS into the VM with vagrant ssh, and exit the SSH session again with Ctrl-d. This leaves the VM running, just vagrant ssh again to get back in. If you modify the Vagrantfile you can use vagrant reload --provision to have Vagrant reload the config and restart the VM. Use vagrant halt to terminate the VM (like powering off) and vagrant destroy to blow it away, losing all data inside. For further information on how to use Vagrant, see vagrant --help.If we want to have a larger cluster, we first need to destroy the old cluster.To do that, we'll run ```vagrant destroy``` to delete the cluster.  This willprompt you to make sure you really want to destroy each virtual machine.  You canoptionally add a -f flag to destroy all of the VMs without prompting.Now that we've got a clean slate, we can edit our config.rb to increase the num_instances variable, and then run ```vagrant udp``` to provision a new cluster.Now can then SSH into the newly created Virtual machiens:```vagrant ssh core-04 -- -Afleetctl list-machinesexitvagrant destroy```and see all 5 CoreOS instances.  In general, unless you have a really powerfulcomputer, I recommend running either 1 or 3 CoreOS instances.  Running threehas the advantage of allowing your development environment more closelyemulate a production environment, but it does require more power than a singlevirtual machine.So that's it for this tutorial.  We have successfully launched three node and fivenode CoreOS clusters on our local computer using Vagrant and VirtualBox.### 0403 systemdIn this video, we are going to learn more about systemd and how it manages theprocesses that run on your CoreOS machines.Systemd is an init system that provides tools for starting, stopping, and managing processes.CoreOS uses systemd as its primary init system, and you will use it to manage your docker containers and other software systems.Before we create our first systemd service, lets go over some basic terminology that I will be using throughout this tutorial series.So systemd has two main concepts: a unit and a target.A unit, is a configuration file that describes the properites of a process that should be run.  With CoreOS, these processes are usually ```docker run``` commands, but any process that you might want to run on CoreOS will be described in a unit file.A target is used to group units together to start them at a common point in time.  systemd is the first process that is started on CoreOS, and it monitors the different targets and starts the processes in each target at the same time.  Most of our units willbelong in the multi-user.targetwhich holds all of the general use units.On CoreOS, unit files are stored in /etc/systemd/system.  Let's create a new unit called helloworld.service.* sudo vi /etc/systemd/system/helloworld.serviceThe Description field shows up in the systemd log and a few other places.  It's helpful to put in a name that will allow you to understand exactly what this service does.The After and Requires lines signify that we want systemd to start docker.service if it hasn't already, and that this helloworld.service should start after docker.service is active.The Service block describes what will run with we start or stop this service.The TimeoutStartSec tells systemd to disable its regular service timout.  Sometimes pulling Docker images can take a while, and we don't want systemd to fail a service simply because the docker pull takes a while.ExecStart, ExecStartPre, and ExecStartPost allow you to specify any command you'd like to run when this unit is started.  Normally, if one of these commands fails, the unit will stop.  You can change this behavior by putting a minus symbol after the equals as in the docker kill and docker rm commands.  The =- will tell systemd to ignore errors from this command.The pid of the process run by the ExecStart is  what systemd will monitor to determine if the process has crashed or not.  Its important to not that you should NOT run docker containers with the ```-d``` flag, as that will run the container in the background.  systemd will think the process has exited and the unit will be stopped.In this unit, we're going to make sure that previous containers with the name hello-world are killed and removed.  Next we pull the latest busybox docker image, then run a busybox container with a bash loop printing Hello World.Finally, we've specified the target of this service as multi-user.target. This means that if the computer restarts, when the multi-user target starts, our hello-world service will also start.So I'm going to save this service, and lets try starting it.systemctl is the command to manage systemd units.  First, we're going to enable our service.  This registers the unit with the multi-user target.systemctl enable helloworld.serviceWe can see the current status of our service withsystemctl status helloworld.serviceNot that our service is currently loaded and enabled.  Lets start our servicesystemctl start helloworld.serviceNow when we request the status we can see that the service is Running, the status codes of the various commands, and finally the last few log entries that our process wrote to standard out.We can tail the logs of our helloworld service using journalctl.journalctl -f -u helloworld.service-f means follow, -u specifies the service we want to examine the logs of.  Now we can seethe hello world message being printed every second.Control-C exits journalctl.  Let's stop our service and see what happens.systemctl status helloworld.serviceNow we can see that the service is failed, having been killed by our stop command.  Finally, we can unload the service, so that it doesn't start on boot.systemctl disable helloworld.servicesystemctl status helloworld.serviceFinally, I want to demonstrate systemd specifiers, which dynamically insert information into your unit.  This is incrediably helpful for creating templated andre-usable units.A common situation you'll encounter is that you want to run multiple instancesof the same service -- perhaps to handle lots of incoming requests.  You couldcreate a unit file for each instance, or you can create a generic unit thatworks for all of your instances.Systemd recognizes templated units as unit files with an @ symbol in the name.Anything before the @ symbol is called the prefix, and anything after the @ but before the postfix (like .service) is called the instance string.For example, lets create a template unit called helloworld2@.service.In this service, we're going to use several systemd specifiers that will beinserted at runtime.  We want to name our docker containers %p-%i, which willtranslate to helloworld2-instance name.  You can also use these specifierswithin your ExecStart commands to pass information into your processes.  hereI'm going to have the echo command print out the unit instance name and the hostname of the machine it is running on.You can see the full list of systemd specifiers in the systemd documentation.http://www.freedesktop.org/software/systemd/man/systemd.unit.html#SpecifiersOk, so now that we've created the template unit file, lets instantiate an instance of it.systemctl start helloworld2@1.serviceThis will start a new unit called helloworld2@1.service, which is an instance ofthe helloworld2 template.  Lets look at the journal for this service:journalctl -f -u helloworld2@1.serviceNote that this service is printing its instance name, as well as the host.We can start multiple of a service using bash expansionsystemctl start helloworld2{2..4}.serviceThis will start instances named 2 through 4.  Not, the instance string doesn'thave to be a number - it is just a generic string.We can see all of the running instances using docker psFinally, lets stop all of the instances that we've started.systemctl stop helloworld@{1..4}.serviceAwesome.  So to recap, we've just learned how to manage processes running on acoreos computer using systemd.  You now know how to get the logs from any runningprocess, how to write unit files and templated unit files.###	0404 fleetctlIn this video, we're going to learn how to use fleetctl to manage runningprocesses across a CoreOS cluster.  As we've previously learned, CoreOSuses systemd as its init system, or process manager on each system.  You can thinkof fleet as a distributed systemd for your entire cluster.  In fact,fleet manages the systemd on each computer in the cluster directly.To run services or processes in your cluster, you must submit regular systemd units with somefleet specific properties.  In this video, we will be using fleetctl locallyon a CoreOS machine.  fleetctl can also manage clusters remotely and we willcover that later in this tutorial series.Before we get started, lets familize ourselves with some common fleet commands.We've already used ```fleetctl list-machines``` to list all of the computers in our cluster.We can also list all of the units that are currently running in our cluster withfleetctl list-units.  We haven't started anything, so there's nothing to see.Finally, we can see all of the unit-files that have been submitted to fleet usingfleet list-unit-files.  Again, we haven't submitting anything, so there's nothing to see here.Next, lets create a new service called hellofleet.service.  This should lookfamiliar to our helloworld.service that we worked with systemd directly.Before we can do anything with this service, we have to submit it to fleet.fleetctl submit hellofleet.servicenow we can list our unit filesfleetctl list-unit-filesFleet is now aware of our hellofleet.service, it hashed the file, and tells us that the state is currently inactive.  Importantly, while this service is in the Fleetsystem, it hasn't been placed into the systemd of a physical host.We can see this because fleetctl list-units still is empty.We can tell fleet to assign this service to a machine by using the load command.fleetctl load hellofleet.service.fleetctl list-units now shows us that hellofleet has been assigned to a machine.We're told which physical machine the service is loaded onto, and that the state is still inactive/dead.Finally, we can start this servicefleetctl start hellofleet.servicefleetctl list-units shows that the service is now running.  Fleetctl provides a wayfor us to get the output of our service, just like journalctl does.fleetctl journal -f hellofleet.servicewe can also get the status of our service withfleetctl status hellofleet.service.we can stop our service with stopfleetctl stop hellofleet.serviceFinally, we can destroy our service to remove it from the fleet systemfleetctl destroy hellofleet.serviceOk, so fleet provides a distributed process manager across your CoreOS clusterFleet also provides some rudimentary scheduling mechanisms as well.  WhatI mean by that, is fleet allows us to specify where units should run withinour cluster.  In some cases, we will want two services to always run on the samephysical machine.  Othertimes, we will want to restrict instances of a templatedunit to never run on the same machine.  This last case is common to providehigh-availability of a service across the cluster.In our next unit, lets create multiple instances of a service and tellfleet to run them on different host machines.vi hellofleet2@.serviceNotice this file looks very similar, except now we've added an X-Fleet sectionat the bottom.  Fleet reads this section to understand how you'd like to scheduleor place this unit across the cluster.In this case, we are saying that this unit will conflict with any other instancesof the service with the same name.  This is signified by the * which acts like a wildcard.So lets submit this service, and then start one instance of it.  fleetctl start hellofleet2@1Its important to note that fleetctl start does an implicit load to assign thisunit to a physical host, then a start.  This is a convienent shortcut so youdon't have to always run load then start.  You should also notice thatI didn't have to specifiy the .service extension.  Fleet is smart enough tolook up in is registry of units to find one with an appropriate name.fleetctl list-unitsOk, so there's our first instance, running on this machine.  Lets start two more.fleetctl start hellofleet2@{2..3}Here we can see that one instance is running on each host.  So what happens if wewant to run a 4th?fleetctl start hellofleet2@4Fleetctl hangs as it waits to find an acceptable host to place this service.  Sinceall of the instances are still running, fleet will just sit there forever waitingfor a valid host to start an instance of this unit.Lets ctrl-c to kill fleetctl. and destroy that instancefleetctl destroy hellofleet2@4Again, we can get the streaming log file from any of these instances:fleetctl journal -f hellofleet2@2Now, lets say we want to make changes to this service.vi hellofleet2@.service--> Change the hello worldIf we try to submit our template unit again, we'll get a warning from fleet.This is telling us that fleet already has a unit file with that name, and that itis different than what we'd like to submit.  In order to update our template, we'llneed to destroy the original unit template and re-submit.fleetctl destroy hellofleet2@.servicefleetctl submit hellofleet2@.serviceWe will have to do the same thing to our instances of the service -- destroyand re-startfleetctl destroy hellofleet2@1fleetctl start hellofleet2@1Now we can see our log file that shows the update servicefleetctl journal -f hellofleet2@1So in this example, we've used the fleet-specific option, Conflicts toschedule our service.  There are a number of other options, which you canfind in the fleet documentation: https://coreos.com/docs/launching-containers/launching/fleet-unit-files/One option in particular that I'd like to highlight is a global service.  Aglobal service, is a fleet unit that will be run on every host in the cluster.This is very helpful for common services like log management, health monitoring,etc.  In my systems, I like to  run services like collectd, logstash, and others using fleet global units.I should note that As of right now, global units do have several disadantages that the CoreOS teamare working on.  Namely, you cannot use fleet journal command to get logsfrom global services.  Additionally, there is no easy way to update your globalservices without causing downtime.  Hopefully by the time you watch this video,these restrictions will be removed.That's the end of this session.  To recap, we've just learned how to use fleet and its commandline tool, fleetctl, to start, stop, and manage processes runningin our CoreOS cluster.###	0405 Dockerized service###	0406 Troubleshooting running container###	0407 Toolbox###	0407 etcd & etcdctl